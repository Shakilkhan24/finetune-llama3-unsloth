# Llama 3 Bengali Fine-tuning Project

Fine-tune Llama 3 for the Bengali language using the Unsloth framework, with support for GGUF export and Ollama deployment.

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py           # Main training script
â”‚   â”œâ”€â”€ save_gguf.py       # Script to save model in GGUF format
â”‚   â””â”€â”€ setup_ollama.sh    # Shell script for Ollama setup and deployment
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/            # Configuration files
â”‚   â”œâ”€â”€ model/             # Model training and inference code
â”‚   â””â”€â”€ utils/             # Utility functions for data processing
â””â”€â”€ README.md              # This file
```

## ğŸš€ Setup
1. Install required packages:
   ```bash
   pip install -r requirements.txt
   ```

2. Ensure you have sufficient disk space for model weights and GGUF formats.

## ğŸ¯ Training
1. Configure training parameters in `src/config/config.py`
2. Run the training script:
   ```bash
   python scripts/train.py
   ```
   This will:
   - Fine-tune the Llama 3 model on Custom data
   - Save checkpoints during training
   - Upload the model to Hugging Face Hub

## GGUF Export and Ollama Deployment
### Save the model in GGUF format
```bash
python scripts/save_gguf.py
```
This creates:
- **Q8_0**: 8-bit quantized version (higher quality, larger size)
- **Q4_K_M**: 4-bit quantized version (smaller, slightly lower quality)
- Ollama Modelfile

### Deploy to Ollama
```bash
chmod +x scripts/setup_ollama.sh
./scripts/setup_ollama.sh
```
This will:
- Install Ollama if not present
- Start the Ollama server
- Create the model in Ollama
- Run a test query

## Using the Model
After deployment, you can interact with the model using:

### ğŸ”¹ Command Line
```bash
ollama run bangla-llama "Your prompt here"
```

### ğŸ”¹ API
```bash
curl http://localhost:11434/api/chat -d '{
    "model": "bangla-llama",
    "messages": [
        {
            "role": "user",
            "content": "Your prompt here"
        }
    ]
}'
```

## ğŸ“¦ Model Versions
- **Q8_0**: 8-bit quantized version (better quality, requires more resources)
- **Q4_K_M**: 4-bit quantized version (smaller, more memory-efficient)
